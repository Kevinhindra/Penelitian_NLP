{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7e929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cosineSimilarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cosineSimilarity.py\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "\n",
    "def process_data(input_text):\n",
    "    data_berita = [input_text.lower()]\n",
    "    token_data = [word_tokenize(text) for text in data_berita]\n",
    "\n",
    "    clear_tokens_punctuation = []\n",
    "    for tokens in token_data:\n",
    "        for token in tokens:\n",
    "            if not re.match(r'^\\d+\\.\\d+$', token):\n",
    "                token = token.translate(str.maketrans('', '', string.punctuation.replace('-', '')))\n",
    "            if token:\n",
    "                clear_tokens_punctuation.append(token)\n",
    "\n",
    "    clear_tokens = []\n",
    "    tanggal_dan_tahun_regex = re.compile(r'^t\\w+')\n",
    "    pukul_dan_periode_regex = re.compile(r'^p\\w+')\n",
    "    jam_regex = re.compile(r'^j\\w+')\n",
    "    menit_regex = re.compile(r'^m\\w+')\n",
    "    detik_regex = re.compile(r'^d\\w+')\n",
    "\n",
    "    i = 0\n",
    "    while i < len(clear_tokens_punctuation):\n",
    "        token = clear_tokens_punctuation[i]\n",
    "        if tanggal_dan_tahun_regex.match(token):\n",
    "            clear_tokens.append(token)\n",
    "            if i + 1 < len(clear_tokens_punctuation) and clear_tokens_punctuation[i + 1].isdigit():\n",
    "                clear_tokens.append(clear_tokens_punctuation[i + 1])\n",
    "        elif pukul_dan_periode_regex.match(token):\n",
    "            clear_tokens.append(token)\n",
    "            if i + 1 < len(clear_tokens_punctuation) and clear_tokens_punctuation[i + 1].isdigit():\n",
    "                clear_tokens.append(clear_tokens_punctuation[i + 1])\n",
    "        elif jam_regex.match(token):\n",
    "            clear_tokens.append(token)\n",
    "            if i + 1 < len(clear_tokens_punctuation) and clear_tokens_punctuation[i + 1].isdigit():\n",
    "                clear_tokens.append(clear_tokens_punctuation[i + 1])\n",
    "        elif menit_regex.match(token):\n",
    "            if i - 1 >= 0 and clear_tokens_punctuation[i - 1].isdigit():\n",
    "                clear_tokens.append(clear_tokens_punctuation[i - 1])\n",
    "            clear_tokens.append(token)\n",
    "        elif detik_regex.match(token):\n",
    "            if i - 1 >= 0 and clear_tokens_punctuation[i - 1].isdigit():\n",
    "                clear_tokens.append(clear_tokens_punctuation[i - 1])\n",
    "            clear_tokens.append(token)\n",
    "        elif token.isdigit():\n",
    "            if i + 1 < len(clear_tokens_punctuation) and tanggal_dan_tahun_regex.match(clear_tokens_punctuation[i + 1]):\n",
    "                clear_tokens.append(token)\n",
    "        elif not token.isdigit():\n",
    "            clear_tokens.append(token)\n",
    "        i += 1\n",
    "    return clear_tokens\n",
    "\n",
    "def keterangan_waktu_yang_benar(file_path):\n",
    "    column_title = 'keterangan_waktu'\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    gram_yang_benar = df[column_title].str.lower().values\n",
    "    \n",
    "    return gram_yang_benar\n",
    "\n",
    "def keterangan_waktu(dataset_path):\n",
    "    one_gram_data = []\n",
    "    two_gram_data = []\n",
    "    \n",
    "    column1_title = 'keterangan_waktu' \n",
    "    column2_title = 'keterangan_waktu_yang_salah' \n",
    "\n",
    "    for dataset_path in dataset_path:\n",
    "        with open(dataset_path, 'r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            for row in reader:\n",
    "                if column1_title in row:\n",
    "                    tokens = row[column1_title].lower().split()\n",
    "                    if len(tokens) == 1:\n",
    "                        one_gram_data.append(tokens[0])\n",
    "                    elif len(tokens) == 2:\n",
    "                        two_gram_data.append(' '.join(tokens))\n",
    "                if column2_title in row:\n",
    "                    tokens = row[column2_title].lower().split()\n",
    "                    if len(tokens) == 1:\n",
    "                        one_gram_data.append(tokens[0])\n",
    "                    elif len(tokens) == 2:\n",
    "                        two_gram_data.append(' '.join(tokens))\n",
    "    \n",
    "    return one_gram_data,two_gram_data\n",
    "    \n",
    "def matching_token(clear_tokens,one_gram_data,two_gram_data):\n",
    "    matching_token_one_grams = set(clear_tokens).intersection(set(one_gram_data))\n",
    "\n",
    "    matching_token_two_grams = set(' '.join([clear_tokens[i], clear_tokens[i+1]]) for i in range(len(clear_tokens)-1)).intersection(set(two_gram_data))\n",
    "            \n",
    "    return matching_token_one_grams,matching_token_two_grams\n",
    "\n",
    "def get_gram_frequency(tokens):\n",
    "    gram_freq = {}\n",
    "    for token in tokens:\n",
    "        if token in gram_freq:\n",
    "            gram_freq[token] += 1\n",
    "        else:\n",
    "            gram_freq[token] = 1\n",
    "    return gram_freq\n",
    "\n",
    "def cosine_similarity(token1, token2):\n",
    "    \n",
    "    gram_freq1 = get_gram_frequency(token1)\n",
    "    gram_freq2 = get_gram_frequency(token2)\n",
    "    \n",
    "    unique_grams = list(set(list(gram_freq1.keys()) + list(gram_freq2.keys())))\n",
    "    vec1 = [gram_freq1.get(gram, 0) for gram in unique_grams]\n",
    "    vec2 = [gram_freq2.get(gram, 0) for gram in unique_grams]\n",
    "    \n",
    "    dot_product = sum([vec1[i]*vec2[i] for i in range(len(vec1))])\n",
    "    vec1_length = math.sqrt(sum([vec1[i]**2 for i in range(len(vec1))]))\n",
    "    vec2_length = math.sqrt(sum([vec2[i]**2 for i in range(len(vec2))]))\n",
    "\n",
    "    if vec1_length == 0 or vec2_length == 0:\n",
    "        cosine_sim = 0\n",
    "    else:\n",
    "        cosine_sim = dot_product / (vec1_length * vec2_length)\n",
    "    return cosine_sim     \n",
    "\n",
    "def main():\n",
    "    st.title(\"Deteksi Keterangan Waktu dengan Algoritma Cosine Similarity\")\n",
    "    input_text = st.text_area(\"Masukkan teks:\")\n",
    "    \n",
    "    if st.button(\"Proses\"):\n",
    "        if input_text:\n",
    "            clear_tokens = process_data(input_text)\n",
    "\n",
    "        else:\n",
    "            st.warning('Tolong masukkan teks!', icon=\"⚠️\")\n",
    "            st.stop()\n",
    "            \n",
    "        file_path = 'D:\\Campus\\Skripsi\\Data Skripsi\\keterangan_waktu_yang_benar.csv'\n",
    "        \n",
    "        gram_yang_benar = keterangan_waktu_yang_benar(file_path)\n",
    "        \n",
    "        dataset_path = ['D:\\Campus\\Skripsi\\Data Skripsi\\keterangan_waktu_yang_benar.csv', \n",
    "                        'D:\\Campus\\Skripsi\\Data Skripsi\\keterangan_waktu_yang_salah.csv']\n",
    "        \n",
    "        one_gram_data,two_gram_data = keterangan_waktu(dataset_path)\n",
    "\n",
    "        matching_token_one_grams,matching_token_two_grams = matching_token(clear_tokens,one_gram_data,two_gram_data)\n",
    "        \n",
    "        combined_text = ' '.join(clear_tokens)\n",
    "        \n",
    "        for token in clear_tokens:\n",
    "            if token in matching_token_one_grams:\n",
    "                # pencarian satu kata dan tidak ada kata belakangnya atau gabungan kata\n",
    "                if re.search(rf'\\b{re.escape(token)}\\b', combined_text) and not re.search(rf'\\b{re.escape(token)}\\w+\\b', combined_text):\n",
    "                    combined_text = combined_text.replace(token, f'<mark style=\"background-color: yellow;\">{token}</mark>')\n",
    "\n",
    "            # pencarian dua kata pada teks\n",
    "            for i in range(len(clear_tokens) - 1):\n",
    "                token = f'{clear_tokens[i]} {clear_tokens[i+1]}'\n",
    "                if token in matching_token_two_grams:\n",
    "                    if re.search(rf'\\b{re.escape(token)}\\b', combined_text) and not re.search(rf'\\b{re.escape(token)}\\w+\\b', combined_text):\n",
    "                        combined_text = combined_text.replace(token, f'<mark style=\"background-color: yellow;\">{token}</mark>')\n",
    "\n",
    "        st.header(\"Teks yang telah diproses:\")\n",
    "        st.markdown(combined_text, unsafe_allow_html=True)\n",
    "            \n",
    "        if matching_token_one_grams is not None and len(matching_token_one_grams) > 0 or matching_token_two_grams is not None and len(matching_token_two_grams) > 0:\n",
    "            if len(matching_token_one_grams) > 0:\n",
    "                \n",
    "                token1 = matching_token_one_grams\n",
    "                token2 = [word for word in gram_yang_benar if len(word.split()) == 1]\n",
    "\n",
    "                threshold = 0.99\n",
    "                below_threshold_count = 0\n",
    "                above_threshold_count = 0\n",
    "                \n",
    "                st.header(\"Cosine Similarity Satu Gram :\")\n",
    "                similarity_results = []\n",
    "                \n",
    "                for t1 in token1:\n",
    "                    max_similarity = 0\n",
    "                    max_token = \"\"\n",
    "                    for t2 in token2:\n",
    "                        similarity = cosine_similarity(t1, t2)\n",
    "                        if similarity > max_similarity:\n",
    "                            max_similarity = similarity\n",
    "                            max_token = t2\n",
    "\n",
    "                    if max_similarity < threshold:\n",
    "                        below_threshold_count += 1\n",
    "                    else:\n",
    "                        above_threshold_count += 1\n",
    "\n",
    "                    similarity_results.append({\n",
    "                        \"Token 1\": t1,\n",
    "                        \"Paling mirip dengan token pada Token 2\": max_token,\n",
    "                        \"Nilai Similarity\": max_similarity\n",
    "                    })\n",
    "                    \n",
    "                similarity_result_one_grams = pd.DataFrame(similarity_results)\n",
    "                \n",
    "                # membuat index mulai dari 1\n",
    "                similarity_result_one_grams.index = similarity_result_one_grams.index + 1\n",
    "                st.dataframe(similarity_result_one_grams, width=500)\n",
    "\n",
    "                st.write(f\"Munculnya di bawah dari batas ({threshold}): {below_threshold_count}\")\n",
    "                st.write(f\"Munculnya di atas batas atau sama dengan ({threshold}): {above_threshold_count}\")\n",
    "                \n",
    "            if len(matching_token_two_grams) > 0:\n",
    "                \n",
    "                token1 = matching_token_two_grams\n",
    "                token2 = [word for word in gram_yang_benar if len(word.split()) == 2]\n",
    "\n",
    "                threshold = 0.99\n",
    "                below_threshold_count = 0\n",
    "                above_threshold_count = 0\n",
    "\n",
    "                st.subheader(\"Cosine Similarity Dua Gram :\")\n",
    "                similarity_results = []\n",
    "                for t1 in token1:\n",
    "                    max_similarity = 0\n",
    "                    max_token = \"\"\n",
    "                    for t2 in token2:\n",
    "                        similarity = cosine_similarity(t1, t2)\n",
    "                        if similarity > max_similarity:\n",
    "                            max_similarity = similarity\n",
    "                            max_token = t2\n",
    "\n",
    "                    if max_similarity < threshold:\n",
    "                        below_threshold_count += 1\n",
    "                    else:\n",
    "                        above_threshold_count += 1\n",
    "\n",
    "                    similarity_results.append({\n",
    "                        \"Token 1\": t1,\n",
    "                        \"Paling mirip dengan token pada Token 2\": max_token,\n",
    "                        \"Nilai Similarity\": max_similarity\n",
    "                    })\n",
    "\n",
    "                similarity_result_two_grams = pd.DataFrame(similarity_results)\n",
    "                \n",
    "                # membuat index mulai dari 1\n",
    "                similarity_result_two_grams.index = similarity_result_two_grams.index + 1\n",
    "                st.dataframe(similarity_result_two_grams, width=500)\n",
    "                \n",
    "                st.write(f\"Munculnya di bawah dari batas ({threshold}): {below_threshold_count}\")\n",
    "                st.write(f\"Munculnya di atas batas atau sama dengan ({threshold}): {above_threshold_count}\")\n",
    "        else:\n",
    "            st.subheader(\"Tidak ada kata keterangan waktu yang ditemukan\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
